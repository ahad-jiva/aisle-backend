# AI.sle

An LLM-powered online shopping agent, built with LangChain.

## Summary
AI.sle is an agentic shopping assistant. You can converse with it naturally to get product recommendations, compare products, and get help choosing the ideal product for your use case. You can also provide images to find products similar to the image content.

## Architecture
AI.sle is built with LangChain. Underneath, it uses Gemini 2.5 Pro for natural text generation, tool calling, and reasoning. I generated two Milvus vector databases using a dataset with 1.4 million Amazon products I found on Kaggle. I developed two tools that the model can use:

### Product Search/Recommendation
The product search tool is a vector database powered recommendation system. I created a Milvus vector database using [this](https://www.kaggle.com/datasets/asaniczka/amazon-products-dataset-2023-1-4m-products?select=amazon_products.csv) dataset. The embedded vectors are generated by the `all-MiniLM-L6-v2` embedding model (chosen for speed). It contains 1.4 million product entries scraped from Amazon in Sept. 2023. Each product entry contains information like name, price, rating, category, etc. When the LLM calls the tool after seeing the user's request, the query gets embedded into a 384-dimension dense vector, then is compared against the existing vectors in the database using cosine similarity. The tool retrieves the six most similar vectors in the database (configurable for more/less) and returns them to the LLM. It also adds the product information in a dedicated response field for the frontend to render nicely in the chat interface.

### Image Search
The image search tool works similarly to the product search tool. In the Amazon product dataset, each entry has an imgURL field containing a link to the product image. Those images are embedded into 512-dimension dense vectors with OpenAI's CLIP model. Due to time and compute constraints, I randomly sampled 25,000 of the 1.4 million items (making sure to get at least 100 from each category) to build the image database. When the LLM calls the image search tool, the query image is embedded with the same CLIP model and compared using cosine similarity to the vectors in the database. The most similar ones are returned and displayed to the user in the same manner as the product search.

Orchestration is handled by LangChain. Depending on the user's query, the LLM can choose to either use the tools available to it, or to directly respond to the user for more general queries.

## API
The backend uses two API endpoints for simplicity.

### /health (GET)
A simple health check to see the status of the backend. The frontend hits this endpoint regularly to check if the agent is available.

Parameters: None

Response:

- status: healthy
- agent: bool (if the model is loaded or not)

### /chat (POST)
The main endpoint used to get chat responses. Called every time a user sends in a message.

Parameters:

- message (str): Contains the user's natural text query
- image_search (optional bool): True if user uploaded an image, false otherwise
- image_data (optional str): Uploaded image data encoded in base64

Response:

- response (str): The agent's natural language reply to the user's message.
- products (optional list[dict]): If the agent used the product search tool, this will be a list of up to 6 products and their info for the frontend to render as card components.

## Usage
Unfortunately, deployment services like Vercel and Render do not offer enough memory on the free tier to deploy this backend. You can deploy it locally with the following instructions:

Step 1:
```bash
pip install -r requirements.txt
```
Step 2: \
Make sure you have a `.env` set up with the right keys (Gemini API key, Zilliz URI, and Zilliz token). Download the dataset from Kaggle and store it in `<root>/data`. Then run the database building scripts:

Step 3:
```bash
python3 build_database.py

python3 build_image_database_fast.py
```

These two files may take a few minutes to run. The embedding process is set up to automatically select compatible CUDA devices or Apple Silicon GPUs (with CPU fallback if necessary).

Step 4: \
Once the vector databases are set up, start the API server:

```bash
python3 run_api.py
```

This will start a uvicorn server that is available locally on port `8000`. The backend is now running, you can test to see if it works with:

```bash
curl -X GET "http://localhost:8000/health"
```

## Future Considerations
Ideally this service could be deployed somewhere for easier use. Should be easy to set up if willing to spend some money.

The current Amazon product database is two years out of date. Updating that would be helpful. Also, adding another tool that allowed the LLM access to the web for searching Amazon or other online stores would allow for real-time updated product information (but the take-home project spec said products are limited to a predefined catalog, so I didn't implement this).

With more time and compute, the image database could be a lot bigger. This would improve the image search feature.

Currently, the agent writes out the whole response before sending it to the user. This gives the impression of long loading times. Implementing some kind of streaming where the response is "written out" could improve the user experience.